from bs4 import BeautifulSoup
import urllib.request
import re, requests

def sub_domain_search():
    www_links = []
    non_www_links = []
    site = 'https://www.google.com'
    html_page = urllib.request.urlopen(site)
    soup = BeautifulSoup(html_page, 'lxml')

    # Scrape for <a> tags that have secure links on the webpage
    for link in soup.findAll('a', attrs={'href': re.compile("^https://")}):
        www_links.append(link.get('href').split('/')[2])

    # Scrape for <a> tags that have non-secure links on the webpage
    for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
        www_links.append(link.get('href').split('/')[2])

    # Filter out links that start with 'www'
    for i in range(len(www_links)):
        if www_links[i][:3] != 'www':
            non_www_links.append(www_links[i])

    # Print each sub-domain
    for i in range(len(non_www_links)):
        print('Sub-domain: {0}, Status Code: {1}'.format(
            non_www_links[i],
            requests.get('https://{}'.format(non_www_links[i])).status_code)
        )

if __name__ == "__main__":
    sub_domain_search()
